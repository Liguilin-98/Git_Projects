{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3AttentionLstmDemo3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMId4A2LFoAuN/gh9LxobQh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Liguilin-98/Git_Projects/blob/master/3AttentionLstmDemo3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 所有导入的包"
      ],
      "metadata": {
        "id": "KazQjS8DUdFq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhjF-RNtYUDj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.serialization import load\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import os\n",
        "from glob import glob\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import FileLink\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import seaborn as sns \n",
        "%matplotlib inline\n",
        "from IPython.display import display, Image\n",
        "import matplotlib.image as mpimg\n",
        "import cv2\n",
        "\n",
        "import itertools \n",
        "\n",
        "import torch.optim as optim\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from torchvision.utils import make_grid\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)\n",
        "print(cv2.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUJn6NesJ6Cs",
        "outputId": "ba732a49-fc15-4fd9-fa94-495f98b3d381"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.11.0+cu113\n",
            "4.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 从kaggle导入数据集"
      ],
      "metadata": {
        "id": "tmHDGKyGUrTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "\n",
        "import json\n",
        "token = {\"username\":\"guilinlie\",\"key\":\"af8933a0ec6dc42a577c0143979119a3\"}\n",
        "with open('/content/kaggle.json', 'w') as file:\n",
        "  json.dump(token, file)\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle config set -n path -v /content\n",
        "\n",
        "!kaggle competitions download -c state-farm-distracted-driver-detection\n",
        "#上面这些步骤无论在kaggle上下载什么数据集都需要重复的\n",
        "#https://blog.csdn.net/qq_20880939/article/details/105613800\n",
        "#\n",
        "\n",
        "!unzip /content/competitions/state-farm-distracted-driver-detection/state-farm-distracted-driver-detection.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zQ08rgfUvOp",
        "outputId": "824ea7a5-ae49-4021-c432-5cb6947866bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2022.5.18.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "- path is now set to: /content\n",
            "state-farm-distracted-driver-detection.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  /content/competitions/state-farm-distracted-driver-detection/state-farm-distracted-driver-detection.zip\n",
            "replace driver_imgs_list.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 数据预处理\n",
        "\n",
        "\n",
        "*   读取数据\n",
        "*   裁剪数据\n",
        "\n"
      ],
      "metadata": {
        "id": "3Q9KinQYVgId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Classes\n",
        "NUMBER_CLASSES = 10\n",
        "\n",
        "\n",
        "activity_map = {'c0': 'Safe driving', \n",
        "                'c1': 'Texting - right', \n",
        "                'c2': 'Talking on the phone - right', \n",
        "                'c3': 'Texting - left', \n",
        "                'c4': 'Talking on the phone - left', \n",
        "                'c5': 'Operating the radio', \n",
        "                'c6': 'Drinking', \n",
        "                'c7': 'Reaching behind', \n",
        "                'c8': 'Hair and makeup', \n",
        "                'c9': 'Talking to passenger'}\n",
        "# Train path\n",
        "train_path = \"/content/imgs/train\"\n",
        "# Test path\n",
        "test_path = \"/content/imgs/test\"\n",
        "\n",
        "train_length = 0\n",
        "for clss in os.listdir(train_path):\n",
        "    print(\"%s size: %d\" % (clss, len(os.listdir(os.path.join(train_path, clss)))))\n",
        "    train_length += len(os.listdir(os.path.join(train_path, clss)))\n",
        "print(\"Train size: %d\" % train_length)\n",
        "print(\"Test Size: %d\" % len(os.listdir(test_path)))"
      ],
      "metadata": {
        "id": "4dNrVsejZbI4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef0bfeb0-617f-49c4-fa7f-c0a6ad51cd55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c0 size: 2489\n",
            "c6 size: 2325\n",
            "c2 size: 2317\n",
            "c9 size: 2129\n",
            "c1 size: 2267\n",
            "c7 size: 2002\n",
            "c3 size: 2346\n",
            "c4 size: 2326\n",
            "c8 size: 1911\n",
            "c5 size: 2312\n",
            "Train size: 22424\n",
            "Test Size: 79726\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Transforms and Augmentation\n",
        "train_transforms = transforms.Compose([ transforms.Resize((64, 64)),\n",
        "                               transforms.Grayscale(num_output_channels=1),\n",
        "                               transforms.RandomAdjustSharpness(2),\n",
        "                               transforms.RandomRotation((-15, 15)),\n",
        "                               transforms.ColorJitter(brightness=.5, hue=.3),\n",
        "                               transforms.ToTensor()\n",
        "                             ])\n",
        "\n",
        "# Loading Data using ImageFolder\n",
        "train_ds = ImageFolder(train_path, train_transforms)\n",
        "classes = train_ds.classes\n",
        "print(classes)\n",
        "\n",
        "# Splitting into train-val set\n",
        "val_pct = .1\n",
        "val_size = int(val_pct * len(train_ds))\n",
        "train_ds ,valid_ds = random_split(train_ds, [len(train_ds)-val_size, val_size])\n",
        "\n",
        "# Data Loader\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_ds, batch_size, shuffle = True, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(valid_ds, batch_size, num_workers=2, pin_memory=True)\n"
      ],
      "metadata": {
        "id": "WWyzXgzZYX6_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "130e71d5-00c7-4116-aa6c-3f84273ebe5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of train samples: ', len(train_ds))\n",
        "img, target = train_ds[3] # load 4th sample\n",
        "\n",
        "print(\"Image Size: \", img.size())\n",
        "print(target)\n",
        "\n",
        "print('Number of test samples: ', len(valid_ds))\n",
        "\n",
        "n_iters = 30000\n",
        "num_epochs = n_iters / (len(train_ds) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "print(\"num_epoch\",num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEvCYqkRIDN0",
        "outputId": "f66e95fd-5849-409b-e268-28ef34937b38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of train samples:  20182\n",
            "Image Size:  torch.Size([1, 64, 64])\n",
            "4\n",
            "Number of test samples:  2242\n",
            "num_epoch 95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "输入输出的各个维度含义为 (seq_length,batch,feature)\n",
        "序列长度，batch，特征\n",
        "长，batch，宽\n",
        "batchfirst=true表示把batch放在第一位，batch，长，宽"
      ],
      "metadata": {
        "id": "DzjV-BxPbh0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "batch_first=False的情况下\n",
        "*   inputlayer_size(seq_len, batch, input_size)\n",
        "*   h0_size(num_layers * num_directions, batch, hidden_size)\n",
        "*   c0_size(num_layers * num_directions, batch, hidden_size)\n",
        "*   outlayer_size(seq_len, batch, num_directions * hidden_size)\n",
        "*   \n",
        "\n"
      ],
      "metadata": {
        "id": "-eO4wsZQiCB9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   batch_first=True的情况输出\n",
        "*   torch.Size([100, 3, 32, 32])//print(image.shape)\n",
        "*   torch.Size([300, 32, 32])//print(x.shape)\n",
        "*   torch.Size([1, 300, 100])//print(h0.shape)\n",
        "*   torch.Size([1, 300, 100])//print(c0.shape)\n",
        "*   torch.Size([300, 32, 100])//print(out.shape)\n",
        "*   torch.Size([1, 300, 100])//print(hn.shape)\n",
        "*   torch.Size([1, 300, 100])//print(cn.shape)\n",
        "*   torch.Size([300, 10])//print(out.shape)\n"
      ],
      "metadata": {
        "id": "P0qs-jsZHiO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        # Hidden dimensions\n",
        "        \n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Number of hidden layers\n",
        "        self.layer_dim = layer_dim\n",
        "        # Building your LSTM\n",
        "        # batch_first=True causes input/output tensors to be of shape\n",
        "        # (batch_dim, seq_dim, feature_dim)\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
        "\n",
        "        # Readout layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def attention(self, lstm_output, final_state):\n",
        "        #print(lstm_output.size()) = (batch_size, squence_length, hidden_size*layer_size)\n",
        "        #torch.Size([100, 32, 100])//print(out.shape)\n",
        "        #hn_size(num_layers * num_directions, batch, hidden_size)\n",
        "        #torch.Size([1, 100, 100])//print(hn.shape)        \n",
        "        hidden = final_state.permute(1, 2, 0)#[batch_size, n_hidden * num_directions, n_layer)] [100,100,1]       \n",
        "        attn_weights = torch.bmm(lstm_output, hidden).squeeze(2) \n",
        "        #attn_weights : [batch_size, n_hidden * num_directions] [100,32]\n",
        "        #假定两个张量的形状分别是(n,a,b)和(n,b,c)， 它们的批量矩阵乘法输出的形状为(n,a,c)。\n",
        "        # .squeeze(2)表示若第三维度为1则去掉第三维度\n",
        "        #attn_tanh = torch.tanh(attn_weights)\n",
        "        #print(attn_tanh.size()) = (squence_length * batch_size, attention_size)\n",
        "        soft_attn_weights = F.softmax(attn_weights, 1)#按列方向上进行softmax\n",
        "        # [batch_size, n_hidden * num_directions(=2), n_step] * [batch_size, n_step, 1] = [batch_size, n_hidden * num_directions(=2), 1]\n",
        "        new_hidden_state = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
        "        return new_hidden_state\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden state with zeros\n",
        "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
        "        #print(x.shape)\n",
        "        # Initialize cell state\n",
        "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
        "        #print(h0.shape)\n",
        "        #print(c0.shape)\n",
        "        # 28 time steps\n",
        "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
        "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
        "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
        "        attn_output = self.attention(out,hn)\n",
        "        #print(out.shape) # torch.Size([300, 32, 100])        \n",
        "        #print(hn.shape)\n",
        "        #print(hn.shape)\n",
        "        # Index hidden state of last time step\n",
        "        # out.size() --> 100, 28, 100\n",
        "        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! \n",
        "        out = self.fc(attn_output.squeeze(0))\n",
        "        \n",
        "        # out.size() --> 100, 10\n",
        "        #print(out.shape)\n",
        "        return out"
      ],
      "metadata": {
        "id": "8r-HpInCdIEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 64\n",
        "hidden_dim = 100\n",
        "layer_dim = 1\n",
        "output_dim = 10"
      ],
      "metadata": {
        "id": "kIgBZZ-rYmJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)"
      ],
      "metadata": {
        "id": "Cz-X_S6UYn08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "learning_rate = 1\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) \n",
        "len(list(model.parameters()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QLyic7oYqAc",
        "outputId": "44843139-e04d-45a5-e93e-655124dfb8d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(list(model.parameters()))):\n",
        "  print(list(model.parameters())[i].size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfUjScd7Yqsx",
        "outputId": "707b3fe6-3f16-40d0-a1f0-31849de2445e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([400, 64])\n",
            "torch.Size([400, 100])\n",
            "torch.Size([400])\n",
            "torch.Size([400])\n",
            "torch.Size([10, 100])\n",
            "torch.Size([10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   images.size()= torch.Size([100, 1, 32, 32]) print('images.size()=',images.size())\n",
        "*   torch.Size([100, 32, 32])\n",
        "*   torch.Size([1, 100, 100])\n",
        "*   torch.Size([1, 100, 100])\n",
        "*   torch.Size([100, 32, 100])\n",
        "*   torch.Size([1, 100, 100])\n",
        "*   torch.Size([1, 100, 100])\n",
        "*   torch.Size([100, 10])"
      ],
      "metadata": {
        "id": "AjUu-n1EybZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of steps to unroll\n",
        "seq_dim = 64  \n",
        "batch = 100\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Load images as a torch tensor with gradient accumulation abilities\n",
        "        #print('images.size()=',images.size())\n",
        "        images = images.view(-1, seq_dim, input_dim).requires_grad_()\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        # outputs.size() --> 100, 10\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "                # Resize images\n",
        "                images = images.view(-1, seq_dim, input_dim)\n",
        "                #images = images.view(100, -1, -1)\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "                # Total correct predictions\n",
        "                correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "\n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJPg_t08YsiZ",
        "outputId": "83e8ba3b-9737-4896-a27d-3fe615e21d19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500. Loss: 2.085143566131592. Accuracy: 22.033897399902344\n",
            "Iteration: 1000. Loss: 1.7504806518554688. Accuracy: 34.12131881713867\n",
            "Iteration: 1500. Loss: 1.2793322801589966. Accuracy: 46.16413879394531\n",
            "Iteration: 2000. Loss: 0.8955131769180298. Accuracy: 59.99108123779297\n",
            "Iteration: 2500. Loss: 0.9913192987442017. Accuracy: 71.94469451904297\n",
            "Iteration: 3000. Loss: 0.5992252230644226. Accuracy: 77.78768920898438\n",
            "Iteration: 3500. Loss: 0.4769154191017151. Accuracy: 82.2033920288086\n",
            "Iteration: 4000. Loss: 0.23884396255016327. Accuracy: 83.63068389892578\n",
            "Iteration: 4500. Loss: 0.4033939242362976. Accuracy: 83.71989440917969\n",
            "Iteration: 5000. Loss: 0.7220348715782166. Accuracy: 87.86797332763672\n",
            "Iteration: 5500. Loss: 0.41306251287460327. Accuracy: 85.14718627929688\n",
            "Iteration: 6000. Loss: 0.2748153507709503. Accuracy: 88.40321350097656\n",
            "Iteration: 6500. Loss: 0.2348773181438446. Accuracy: 91.92684936523438\n",
            "Iteration: 7000. Loss: 0.15133889019489288. Accuracy: 91.48081970214844\n",
            "Iteration: 7500. Loss: 0.4212683439254761. Accuracy: 91.43621826171875\n",
            "Iteration: 8000. Loss: 0.11366664618253708. Accuracy: 93.93399047851562\n",
            "Iteration: 8500. Loss: 0.241025909781456. Accuracy: 92.19446563720703\n",
            "Iteration: 9000. Loss: 0.18282990157604218. Accuracy: 91.57003021240234\n",
            "Iteration: 9500. Loss: 0.12238723784685135. Accuracy: 94.023193359375\n",
            "Iteration: 10000. Loss: 0.188297301530838. Accuracy: 94.6922378540039\n",
            "Iteration: 10500. Loss: 0.20731818675994873. Accuracy: 94.33541107177734\n",
            "Iteration: 11000. Loss: 0.10671637952327728. Accuracy: 94.60303497314453\n",
            "Iteration: 11500. Loss: 0.10154062509536743. Accuracy: 94.20160675048828\n",
            "Iteration: 12000. Loss: 0.2613334059715271. Accuracy: 95.58429718017578\n",
            "Iteration: 12500. Loss: 0.21336136758327484. Accuracy: 95.5396957397461\n",
            "Iteration: 13000. Loss: 0.11945395171642303. Accuracy: 95.4950942993164\n",
            "Iteration: 13500. Loss: 0.10615088045597076. Accuracy: 95.98572540283203\n",
            "Iteration: 14000. Loss: 0.1324402242898941. Accuracy: 95.22747802734375\n",
            "Iteration: 14500. Loss: 0.15935513377189636. Accuracy: 95.13826751708984\n",
            "Iteration: 15000. Loss: 0.07120586186647415. Accuracy: 95.89652252197266\n",
            "Iteration: 15500. Loss: 0.03280670940876007. Accuracy: 96.83318328857422\n",
            "Iteration: 16000. Loss: 0.04257945343852043. Accuracy: 96.11953735351562\n",
            "Iteration: 16500. Loss: 0.12486285716295242. Accuracy: 95.13826751708984\n",
            "Iteration: 17000. Loss: 0.03142043948173523. Accuracy: 95.85192108154297\n",
            "Iteration: 17500. Loss: 0.04552830383181572. Accuracy: 96.43175506591797\n",
            "Iteration: 18000. Loss: 0.09753726422786713. Accuracy: 95.98572540283203\n",
            "Iteration: 18500. Loss: 0.054230641573667526. Accuracy: 96.83318328857422\n",
            "Iteration: 19000. Loss: 0.024655506014823914. Accuracy: 96.16413879394531\n",
            "Iteration: 19500. Loss: 0.14631222188472748. Accuracy: 96.16413879394531\n",
            "Iteration: 20000. Loss: 0.13540183007717133. Accuracy: 95.5396957397461\n",
            "Iteration: 20500. Loss: 0.04634791612625122. Accuracy: 95.98572540283203\n",
            "Iteration: 21000. Loss: 0.06898122280836105. Accuracy: 93.75557708740234\n",
            "Iteration: 21500. Loss: 0.14184251427650452. Accuracy: 94.42462158203125\n",
            "Iteration: 22000. Loss: 0.12303971499204636. Accuracy: 94.6922378540039\n",
            "Iteration: 22500. Loss: 0.045194193720817566. Accuracy: 96.83318328857422\n",
            "Iteration: 23000. Loss: 0.007679585833102465. Accuracy: 97.54683685302734\n",
            "Iteration: 23500. Loss: 0.01506814919412136. Accuracy: 97.19001007080078\n",
            "Iteration: 24000. Loss: 0.15565019845962524. Accuracy: 95.71810913085938\n",
            "Iteration: 24500. Loss: 0.06833674013614655. Accuracy: 94.60303497314453\n",
            "Iteration: 25000. Loss: 0.2526038885116577. Accuracy: 93.44335174560547\n",
            "Iteration: 25500. Loss: 0.06110650673508644. Accuracy: 96.61016845703125\n",
            "Iteration: 26000. Loss: 0.19157375395298004. Accuracy: 94.60303497314453\n",
            "Iteration: 26500. Loss: 0.06685902178287506. Accuracy: 95.89652252197266\n",
            "Iteration: 27000. Loss: 0.09365855902433395. Accuracy: 96.3425521850586\n",
            "Iteration: 27500. Loss: 0.05956897884607315. Accuracy: 95.22747802734375\n",
            "Iteration: 28000. Loss: 0.07815735042095184. Accuracy: 96.52096557617188\n",
            "Iteration: 28500. Loss: 0.04748611897230148. Accuracy: 96.38715362548828\n",
            "Iteration: 29000. Loss: 0.025369901210069656. Accuracy: 96.208740234375\n",
            "Iteration: 29500. Loss: 0.036725003272295. Accuracy: 97.32381439208984\n",
            "Iteration: 30000. Loss: 0.03934217244386673. Accuracy: 97.05619812011719\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.size())\n",
        "print(images.size())\n",
        "print(labels.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCKZ61dJZ5OW",
        "outputId": "8b7414fc-0611-4999-dfa6-5f1be43ca1dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([22, 10])\n",
            "torch.Size([22, 64, 64])\n",
            "torch.Size([22])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)\n",
        "torch.save(model.state_dict(), 'model_dict.pt')\n",
        "torch.save(model,'model_all.pt')"
      ],
      "metadata": {
        "id": "EsoiFz_QGbV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 更新混淆矩阵\n",
        "def confusion_matrix(preds, labels, conf_matrix):\n",
        "    for p, t in zip(preds, labels):\n",
        "        conf_matrix[p, t] += 1\n",
        "    return conf_matrix"
      ],
      "metadata": {
        "id": "HGLDa4RsYYYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "'''\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    Input\n",
        "    - cm : 计算出的混淆矩阵的值\n",
        "    - classes : 混淆矩阵中每一行每一列对应的列\n",
        "    - normalize : True:显示百分比, False:显示个数\n",
        "'''\n",
        "\n",
        "# 绘制混淆矩阵\n",
        "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "\n",
        "  if normalize:\n",
        "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    print(\"Normalized confusion matrix\")\n",
        "    \n",
        "  else:\n",
        "    print('Confusion matrix, without normalization')\n",
        "\n",
        "  print(cm)\n",
        "  plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "  plt.title(title)\n",
        "  plt.colorbar()\n",
        "  tick_marks = np.arange(len(classes))\n",
        "  plt.xticks(tick_marks, classes, rotation=90)\n",
        "  plt.yticks(tick_marks, classes)\n",
        "  \n",
        "# 。。。。。。。。。。。。新增代码开始处。。。。。。。。。。。。。。。。\n",
        "# x,y轴长度一致(问题1解决办法）\n",
        "  plt.axis(\"equal\")\n",
        "  # x轴处理一下，如果x轴或者y轴两边有空白的话(问题2解决办法）\n",
        "  ax = plt.gca()  # 获得当前axis\n",
        "  left, right = plt.xlim()  # 获得x轴最大最小值\n",
        "  ax.spines['left'].set_position(('data', left))\n",
        "  ax.spines['right'].set_position(('data', right))\n",
        "  for edge_i in ['top', 'bottom', 'right', 'left']:\n",
        "      ax.spines[edge_i].set_edgecolor(\"white\")\n",
        "# 。。。。。。。。。。。。新增代码结束处。。。。。。。。。。。。。。。。\n",
        "\n",
        "  thresh = cm.max() / 2.\n",
        "  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "      num = '{:.2f}'.format(cm[i, j]) if normalize else int(cm[i, j])\n",
        "      plt.text(i, j, num,\n",
        "                verticalalignment='center',\n",
        "                horizontalalignment=\"center\",\n",
        "                color=\"white\" if num > thresh else \"black\")\n",
        "  plt.tight_layout()\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "ih0InL1_YZXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建一个空矩阵存储混淆矩阵\n",
        "conf_matrix = torch.zeros(10, 10)\n",
        "for batch_images, batch_labels in test_loader:\n",
        "   print(batch_labels)\n",
        "   print(batch_images.size)\n",
        "   batch_images = batch_images.view(-1, 64, 64).requires_grad_()\n",
        "\n",
        "\n",
        "   out = model(batch_images)\n",
        "\n",
        "   prediction = torch.max(out, 1)[1]\n",
        "   conf_matrix = confusion_matrix(prediction, labels=batch_labels, conf_matrix=conf_matrix)\n",
        "\n",
        "# conf_matrix需要是numpy格式\n",
        "# attack_types是分类实验的类别，eg：attack_types = ['Normal', 'DoS', 'Probe', 'R2L', 'U2R']\n",
        "attack_types = ['c0','c1','c2','c3','c4','c5','c6','c7','c8','c9']\n",
        "plot_confusion_matrix(conf_matrix.numpy(), classes=attack_types, normalize=False,\n",
        "                                 title='Normalized confusion matrix')\n"
      ],
      "metadata": {
        "id": "rPFIIAWFYbq4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}